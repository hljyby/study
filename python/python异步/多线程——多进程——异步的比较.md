# python爬虫进程池，多线程，异步，的使用

前几天写过一个简单的爬取小说的代码，刚好用来测试下使用多进程，多线程以及异步请求的效果
首先不加进程和线程

```python
# -*- coding: utf-8 -*-
# @Author   : LMD
# @FILE     : 重生嫡女：指腹为婚.py
# @Time     : 2019/12/11 11:39
# @Software : PyCharm
import requests
from bs4 import BeautifulSoup
import time
import random

def get_list():
    url='http://mm.hengyan.com/dir/9177.aspx'
    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36',
        }
    rq=requests.get(url,headers=headers)
    rq.encoding='utf8'
    soup=BeautifulSoup(rq.text,'lxml')
    # print(soup)
    company_div=soup.find('div',class_='chapter').findAll('li')[:96]
    links=[]
    for x in company_div:
        hearf=x.find('a')['href']
        link='http://mm.hengyan.com'+hearf
        links.append(link)
    return links

def get_massage(links):
    # with open("dinv_url.txt", "r+", encoding="utf-8") as f:
    #     content = f.read()
    #     url_list = content[:-1].split(',')
    for link in links:
    #     if link not in url_list:
            print(link)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36',
            }
            rq = requests.get(link, headers=headers)
            rq.encoding = 'utf8'
            soup = BeautifulSoup(rq.text, 'lxml')
            # print(soup)
            title=soup.find('div',class_='ch').find('h2').get_text()+'\n'
            # print(title)
            content=soup.find('div',class_='content').get_text()
            # print(content)
            #time.sleep(random.choice(range(1,3)))
            with open("result.txt", "a+", encoding="utf-8") as f:
                f.write(title+ "\n"+content+'\n'+ "\n")
            print(title,'已下载')
            with open("dinv_url.txt", "a+", encoding="utf-8") as f:
                f.write(link + ",")

if __name__=='__main__':
   
    t1=time.time()
    links=get_list()
    # print(links)
    get_massage(links)

    t2=time.time()
    print(t2-t1)

```

用时167秒多
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191227094155120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpcGFjaG9uZw==,size_16,color_FFFFFF,t_70)
接下来加入进程池

```python
# -*- coding: utf-8 -*-
# @Author   : LMD
# @FILE     : 重生嫡女：指腹为婚.py
# @Time     : 2019/12/11 11:39
# @Software : PyCharm
import requests
from bs4 import BeautifulSoup
import time
import random
import multiprocessing
from multiprocessing import Pool

def get_list():
    url='http://mm.hengyan.com/dir/9177.aspx'
    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36',
        }
    rq=requests.get(url,headers=headers)
    rq.encoding='utf8'
    soup=BeautifulSoup(rq.text,'lxml')
    # print(soup)
    company_div=soup.find('div',class_='chapter').findAll('li')[:96]
    links=[]
    for x in company_div:
        hearf=x.find('a')['href']
        link='http://mm.hengyan.com'+hearf
        links.append(link)
    return links

def get_massage(link):
    # with open("dinv_url.txt", "r+", encoding="utf-8") as f:
    #     content = f.read()
    #     url_list = content[:-1].split(',')
    #for link in links:
    #     if link not in url_list:
            print(link)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36',
            }
            rq = requests.get(link, headers=headers)
            rq.encoding = 'utf8'
            soup = BeautifulSoup(rq.text, 'lxml')
            # print(soup)
            title=soup.find('div',class_='ch').find('h2').get_text()+'\n'
            # print(title)
            content=soup.find('div',class_='content').get_text()
            # print(content)
            #time.sleep(random.choice(range(1,3)))
            with open("result.txt", "a+", encoding="utf-8") as f:
                f.write(title+ "\n"+content+'\n'+ "\n")
            print(title,'已下载')
            with open("dinv_url.txt", "a+", encoding="utf-8") as f:
                f.write(link + ",")

if __name__=='__main__':
	t1=time.time()
    links=get_list()
    # print(links)
    # get_massage(links)
    pool = Pool(10)
    pool.map(get_massage, links)
    pool.close()
    pool.join()
    t2=time.time()
    print(t2-t1)
```

可以看到用时22秒多，缩短了太多，如果数据量更大，差距会更加明显。
但是打印顺序是乱的，其实很好理解，有的进程跑得慢，有的进程跑得快。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191227094642270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpcGFjaG9uZw==,size_16,color_FFFFFF,t_70)
下面再改线程看下

```python
# -*- coding: utf-8 -*-
# @Author   : LMD
# @FILE     : 重生嫡女：指腹为婚.py
# @Time     : 2019/12/11 11:39
# @Software : PyCharm
import requests
from bs4 import BeautifulSoup
import time
import random
from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED

def get_list():
    url='http://mm.hengyan.com/dir/9177.aspx'
    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36',
        }
    rq=requests.get(url,headers=headers)
    rq.encoding='utf8'
    soup=BeautifulSoup(rq.text,'lxml')
    # print(soup)
    company_div=soup.find('div',class_='chapter').findAll('li')[:96]
    links=[]
    for x in company_div:
        hearf=x.find('a')['href']
        link='http://mm.hengyan.com'+hearf
        links.append(link)
    return links

def get_massage(link):
    # with open("dinv_url.txt", "r+", encoding="utf-8") as f:
    #     content = f.read()
    #     url_list = content[:-1].split(',')
    # for link in links:
    #     if link not in url_list:
            print(link)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36',
            }
            rq = requests.get(link, headers=headers)
            rq.encoding = 'utf8'
            soup = BeautifulSoup(rq.text, 'lxml')
            # print(soup)
            title=soup.find('div',class_='ch').find('h2').get_text()+'\n'
            # print(title)
            content=soup.find('div',class_='content').get_text()
            # print(content)
            #time.sleep(random.choice(range(1,3)))
            with open("result.txt", "a+", encoding="utf-8") as f:
                f.write(title+ "\n"+content+'\n'+ "\n")
            print(title,'已下载')
            with open("dinv_url.txt", "a+", encoding="utf-8") as f:
                f.write(link + ",")

if __name__=='__main__':
    t1=time.time()
    links=get_list()
    # print(links)
    # get_massage(links)
    executor = ThreadPoolExecutor(max_workers=10)  # 可以自己调整max_workers,即线程的个数
    # submit()的参数： 第一个为函数， 之后为该函数的传入参数，允许有多个
    future_tasks = [executor.submit(get_massage, url) for url in links]
    # 等待所有的线程完成，才进入后续的执行
    wait(future_tasks, return_when=ALL_COMPLETED)

    t2=time.time()
    print(t2-t1)
```

可以看到会更快一些，但是进程和线程适合的场景不同，倒是不好比较。
不过无论怎样，加了进程和线程还是能大大提高抓取速度
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191227095344309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpcGFjaG9uZw==,size_16,color_FFFFFF,t_70)
最后，异步请求

```python
import asyncio
import aiohttp
import time
import requests
from bs4 import BeautifulSoup
start = time.time()
async def get(url):
    session = aiohttp.ClientSession()
    response = await session.get(url)
    result = await response.text()
    await session.close()
    return result
async def request(url):
    print('Waiting for', url)
    result = await get(url)
    # print('Get response from', url, 'Result:', result)
    soup = BeautifulSoup(result, 'lxml')
    # print(soup)
    title = soup.find('div', class_='ch').find('h2').get_text() + '\n'
    # print(title)
    content = soup.find('div', class_='content').get_text()
    # print(content)
    # time.sleep(random.choice(range(1, 3)))
    with open("result.txt", "a+", encoding="utf-8") as f:
        f.write(title + "\n" + content + '\n' + "\n")
    print(title, '已下载')

def get_list():
    url='http://mm.hengyan.com/dir/9177.aspx'
    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36',
        }
    rq=requests.get(url,headers=headers)
    rq.encoding='utf8'
    soup=BeautifulSoup(rq.text,'lxml')
    # print(soup)
    company_div=soup.find('div',class_='chapter').findAll('li')[:96]
    links=[]
    for x in company_div:
        hearf=x.find('a')['href']
        link='http://mm.hengyan.com'+hearf
        links.append(link)
    return links

links=get_list()
tasks = [asyncio.ensure_future(request(x)) for x in links]
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
end = time.time()
print('Cost time:', end - start)

```

简直可以用恐怖形容，当然这里我们没有规定最大请求数，而进程和线程我们都限制了10条，但是异步请求的速度还是真的可观
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191227111536442.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpcGFjaG9uZw==,size_16,color_FFFFFF,t_70)
总结，
应该根据适合场景选择单进程，多进程，多线程，异步。
本文章爬取的小说放在了一个文件里，所以使用多进程，多线程，异步，会导致文章顺序不对。当然，如果每章存一个文件就没事了。

也就是说本文使用的多进程，多线程，异步爬虫，适合没有顺序要求的爬取需求，比如你爬取楼盘网，哪个楼盘信息先入库其实问题不大吧

最后任务量少的时候，乖乖用单进程，省的被封

任务量多的时候可以考虑使用，因为任务多，值得我们搞一些稳定的代理，也就不担心速度快被封了。

至于异步，我试着加代理ip总是不成功，好像是不支持加代理ip还是不支持加https类型的代理ip来着